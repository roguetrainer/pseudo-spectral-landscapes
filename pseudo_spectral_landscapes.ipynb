{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-Spectral Landscapes: Understanding Neural Network Optimization\n",
    "\n",
    "This notebook demonstrates **pseudo-spectral landscapes** and their relevance to understanding Large Language Models (LLMs) through geometric approaches.\n",
    "\n",
    "## What You'll Learn\n",
    "\n",
    "1. **Loss landscape geometry** through Hessian analysis\n",
    "2. **Eigenvalue spectra** and what they reveal\n",
    "3. **Sharp vs flat minima** and generalization\n",
    "4. **Attention mechanism geometry** in transformers\n",
    "5. **Practical implications** for training LLMs\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "from scipy.linalg import eigh, svd\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# For better figure quality\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "print(\"✓ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Toy Neural Network\n",
    "\n",
    "We start with a simple 2-layer neural network. While small, it demonstrates all the key concepts that apply to billion-parameter LLMs.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Hessian Matrix**: $H_{ij} = \\frac{\\partial^2 L}{\\partial \\theta_i \\partial \\theta_j}$\n",
    "- **Eigenvalue Spectrum**: The set of eigenvalues $\\{\\lambda_1, \\lambda_2, ..., \\lambda_n\\}$\n",
    "- **Geometric Interpretation**: Eigenvalues reveal local curvature of the loss landscape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyNeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple 2-layer neural network for demonstration.\n",
    "    Small enough to compute full Hessian, yet rich enough to show key concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_dim=2, hidden_dim=3, output_dim=1, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Initialize weights\n",
    "        self.W1 = np.random.randn(input_dim, hidden_dim) * 0.5\n",
    "        self.b1 = np.random.randn(hidden_dim) * 0.1\n",
    "        self.W2 = np.random.randn(hidden_dim, output_dim) * 0.5\n",
    "        self.b2 = np.random.randn(output_dim) * 0.1\n",
    "        \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, x):\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        self.z1 = X @ self.W1 + self.b1\n",
    "        self.a1 = self.sigmoid(self.z1)\n",
    "        self.z2 = self.a1 @ self.W2 + self.b2\n",
    "        return self.z2\n",
    "    \n",
    "    def get_params_vector(self):\n",
    "        \"\"\"Flatten all parameters into a single vector\"\"\"\n",
    "        return np.concatenate([\n",
    "            self.W1.flatten(),\n",
    "            self.b1.flatten(),\n",
    "            self.W2.flatten(),\n",
    "            self.b2.flatten()\n",
    "        ])\n",
    "    \n",
    "    def set_params_vector(self, params):\n",
    "        \"\"\"Set parameters from a flattened vector\"\"\"\n",
    "        idx = 0\n",
    "        \n",
    "        w1_size = self.input_dim * self.hidden_dim\n",
    "        self.W1 = params[idx:idx + w1_size].reshape(self.input_dim, self.hidden_dim)\n",
    "        idx += w1_size\n",
    "        \n",
    "        self.b1 = params[idx:idx + self.hidden_dim]\n",
    "        idx += self.hidden_dim\n",
    "        \n",
    "        w2_size = self.hidden_dim * self.output_dim\n",
    "        self.W2 = params[idx:idx + w2_size].reshape(self.hidden_dim, self.output_dim)\n",
    "        idx += w2_size\n",
    "        \n",
    "        self.b2 = params[idx:idx + self.output_dim]\n",
    "    \n",
    "    def loss(self, X, y):\n",
    "        \"\"\"MSE loss\"\"\"\n",
    "        pred = self.forward(X)\n",
    "        return 0.5 * np.mean((pred - y) ** 2)\n",
    "    \n",
    "    def compute_gradient(self, X, y):\n",
    "        \"\"\"Compute gradient via backpropagation\"\"\"\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Forward pass\n",
    "        pred = self.forward(X)\n",
    "        \n",
    "        # Backward pass\n",
    "        dz2 = (pred - y) / m\n",
    "        dW2 = self.a1.T @ dz2\n",
    "        db2 = np.sum(dz2, axis=0)\n",
    "        \n",
    "        da1 = dz2 @ self.W2.T\n",
    "        dz1 = da1 * self.sigmoid_derivative(self.z1)\n",
    "        dW1 = X.T @ dz1\n",
    "        db1 = np.sum(dz1, axis=0)\n",
    "        \n",
    "        return np.concatenate([\n",
    "            dW1.flatten(),\n",
    "            db1.flatten(),\n",
    "            dW2.flatten(),\n",
    "            db2.flatten()\n",
    "        ])\n",
    "    \n",
    "    def compute_hessian(self, X, y):\n",
    "        \"\"\"\n",
    "        Compute the Hessian matrix numerically using finite differences.\n",
    "        This is the KEY for pseudo-spectral analysis!\n",
    "        \"\"\"\n",
    "        n_params = len(self.get_params_vector())\n",
    "        H = np.zeros((n_params, n_params))\n",
    "        eps = 1e-5\n",
    "        \n",
    "        def loss_fn(params):\n",
    "            self.set_params_vector(params)\n",
    "            return self.loss(X, y)\n",
    "        \n",
    "        params = self.get_params_vector()\n",
    "        \n",
    "        for i in range(n_params):\n",
    "            for j in range(i, n_params):\n",
    "                # Compute second derivative using finite differences\n",
    "                params_pp = params.copy()\n",
    "                params_pp[i] += eps\n",
    "                params_pp[j] += eps\n",
    "                \n",
    "                params_pm = params.copy()\n",
    "                params_pm[i] += eps\n",
    "                params_pm[j] -= eps\n",
    "                \n",
    "                params_mp = params.copy()\n",
    "                params_mp[i] -= eps\n",
    "                params_mp[j] += eps\n",
    "                \n",
    "                params_mm = params.copy()\n",
    "                params_mm[i] -= eps\n",
    "                params_mm[j] -= eps\n",
    "                \n",
    "                f_pp = loss_fn(params_pp)\n",
    "                f_pm = loss_fn(params_pm)\n",
    "                f_mp = loss_fn(params_mp)\n",
    "                f_mm = loss_fn(params_mm)\n",
    "                \n",
    "                H[i, j] = (f_pp - f_pm - f_mp + f_mm) / (4 * eps * eps)\n",
    "                H[j, i] = H[i, j]\n",
    "        \n",
    "        # Reset to original parameters\n",
    "        self.set_params_vector(params)\n",
    "        return H\n",
    "\n",
    "print(\"✓ ToyNeuralNetwork class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_synthetic_data(n_samples=100, seed=42):\n",
    "    \"\"\"Generate simple synthetic data for demonstration\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    X = np.random.randn(n_samples, 2)\n",
    "    # Simple nonlinear function\n",
    "    y = (np.sin(X[:, 0]) + 0.5 * X[:, 1]**2).reshape(-1, 1)\n",
    "    y += np.random.randn(n_samples, 1) * 0.1  # Add noise\n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = generate_synthetic_data(n_samples=50)\n",
    "print(f\"✓ Generated data: X.shape={X.shape}, y.shape={y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2: Loss Landscape Visualization\n",
    "\n",
    "Let's visualize the loss landscape - a 2D slice through high-dimensional parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loss_landscape_2d(nn, X, y, param_indices=(0, 1), \n",
    "                                 range_scale=2.0, resolution=50):\n",
    "    \"\"\"\n",
    "    Visualize the loss landscape along two parameter directions.\n",
    "    \"\"\"\n",
    "    params = nn.get_params_vector()\n",
    "    i, j = param_indices\n",
    "    \n",
    "    # Create grid around current parameters\n",
    "    param_range_i = np.linspace(params[i] - range_scale, \n",
    "                                 params[i] + range_scale, resolution)\n",
    "    param_range_j = np.linspace(params[j] - range_scale, \n",
    "                                 params[j] + range_scale, resolution)\n",
    "    \n",
    "    loss_grid = np.zeros((resolution, resolution))\n",
    "    \n",
    "    for idx_i, pi in enumerate(param_range_i):\n",
    "        for idx_j, pj in enumerate(param_range_j):\n",
    "            params_temp = params.copy()\n",
    "            params_temp[i] = pi\n",
    "            params_temp[j] = pj\n",
    "            nn.set_params_vector(params_temp)\n",
    "            loss_grid[idx_j, idx_i] = nn.loss(X, y)\n",
    "    \n",
    "    # Reset parameters\n",
    "    nn.set_params_vector(params)\n",
    "    \n",
    "    return param_range_i, param_range_j, loss_grid\n",
    "\n",
    "# Create network and visualize\n",
    "nn = ToyNeuralNetwork()\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "\n",
    "# 2D Contour plot\n",
    "ax1 = plt.subplot(1, 3, 1)\n",
    "param_i, param_j, loss_grid = visualize_loss_landscape_2d(\n",
    "    nn, X, y, param_indices=(0, 1), range_scale=1.5, resolution=40\n",
    ")\n",
    "contour = ax1.contour(param_i, param_j, loss_grid, levels=20, cmap='viridis')\n",
    "ax1.contourf(param_i, param_j, loss_grid, levels=20, cmap='viridis', alpha=0.6)\n",
    "plt.colorbar(contour, ax=ax1)\n",
    "params = nn.get_params_vector()\n",
    "ax1.plot(params[0], params[1], 'r*', markersize=15, label='Current position')\n",
    "ax1.set_xlabel('Parameter 0 (W1[0,0])')\n",
    "ax1.set_ylabel('Parameter 1 (W1[0,1])')\n",
    "ax1.set_title('Loss Landscape (2D Slice)', fontsize=12, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# 3D Surface plot\n",
    "ax2 = plt.subplot(1, 3, 2, projection='3d')\n",
    "Pi, Pj = np.meshgrid(param_i, param_j)\n",
    "surf = ax2.plot_surface(Pi, Pj, loss_grid, cmap='viridis', alpha=0.8)\n",
    "ax2.set_xlabel('Parameter 0')\n",
    "ax2.set_ylabel('Parameter 1')\n",
    "ax2.set_zlabel('Loss')\n",
    "ax2.set_title('3D Loss Surface', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Different slice\n",
    "ax3 = plt.subplot(1, 3, 3)\n",
    "param_i2, param_j2, loss_grid2 = visualize_loss_landscape_2d(\n",
    "    nn, X, y, param_indices=(2, 3), range_scale=1.5, resolution=40\n",
    ")\n",
    "contour2 = ax3.contour(param_i2, param_j2, loss_grid2, levels=20, cmap='plasma')\n",
    "ax3.contourf(param_i2, param_j2, loss_grid2, levels=20, cmap='plasma', alpha=0.6)\n",
    "plt.colorbar(contour2, ax=ax3)\n",
    "ax3.plot(params[2], params[3], 'r*', markersize=15)\n",
    "ax3.set_xlabel('Parameter 2 (W1[0,2])')\n",
    "ax3.set_ylabel('Parameter 3 (W1[1,0])')\n",
    "ax3.set_title('Loss Landscape (Different Slice)', fontsize=12, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Loss landscape visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3: Hessian Spectrum Analysis\n",
    "\n",
    "Now for the **key concept**: analyzing the eigenvalue spectrum of the Hessian matrix.\n",
    "\n",
    "### What the Eigenvalues Tell Us:\n",
    "- **λ > 0**: Upward curvature (local minimum in this direction)\n",
    "- **λ < 0**: Downward curvature (saddle point)\n",
    "- **|λ| ≈ 0**: Flat direction (easy to move without changing loss)\n",
    "- **|λ| large**: Sharp curvature (dominates optimization difficulty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_hessian_spectrum(nn, X, y):\n",
    "    \"\"\"\n",
    "    Compute and analyze the eigenvalue spectrum of the Hessian.\n",
    "    This is the CORE of pseudo-spectral analysis!\n",
    "    \"\"\"\n",
    "    print(\"Computing Hessian (this may take a moment)...\")\n",
    "    H = nn.compute_hessian(X, y)\n",
    "    \n",
    "    # Compute eigenvalues\n",
    "    eigenvalues, eigenvectors = eigh(H)\n",
    "    \n",
    "    return eigenvalues, eigenvectors, H\n",
    "\n",
    "# Analyze the spectrum\n",
    "eigenvalues, eigenvectors, H = analyze_hessian_spectrum(nn, X, y)\n",
    "\n",
    "print(f\"\\n✓ Hessian analysis complete!\")\n",
    "print(f\"   Number of parameters: {len(eigenvalues)}\")\n",
    "print(f\"   Max eigenvalue: {np.max(eigenvalues):.4f}\")\n",
    "print(f\"   Min eigenvalue: {np.min(eigenvalues):.4f}\")\n",
    "print(f\"   Negative eigenvalues: {np.sum(eigenvalues < 0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Eigenvalue Spectrum\n",
    "ax = axes[0, 0]\n",
    "ax.bar(range(len(eigenvalues)), eigenvalues, color='steelblue', alpha=0.7)\n",
    "ax.axhline(y=0, color='r', linestyle='--', alpha=0.5)\n",
    "ax.set_xlabel('Eigenvalue Index')\n",
    "ax.set_ylabel('Eigenvalue Magnitude')\n",
    "ax.set_title('Hessian Eigenvalue Spectrum', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics\n",
    "n_positive = np.sum(eigenvalues > 1e-6)\n",
    "n_negative = np.sum(eigenvalues < -1e-6)\n",
    "ax.text(0.05, 0.95, f'Positive: {n_positive}\\nNegative: {n_negative}\\nZero: {len(eigenvalues)-n_positive-n_negative}',\n",
    "        transform=ax.transAxes, verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# 2. Eigenvalue Distribution\n",
    "ax = axes[0, 1]\n",
    "ax.hist(eigenvalues, bins=15, color='coral', alpha=0.7, edgecolor='black')\n",
    "ax.axvline(x=0, color='r', linestyle='--', linewidth=2, label='Zero')\n",
    "ax.set_xlabel('Eigenvalue')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Eigenvalue Distribution', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Hessian Matrix Heatmap\n",
    "ax = axes[0, 2]\n",
    "im = ax.imshow(H, cmap='RdBu_r', aspect='auto', vmin=-np.abs(H).max(), vmax=np.abs(H).max())\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_xlabel('Parameter Index')\n",
    "ax.set_ylabel('Parameter Index')\n",
    "ax.set_title('Hessian Matrix', fontweight='bold')\n",
    "\n",
    "# 4. Loss along principal curvature directions\n",
    "ax = axes[1, 0]\n",
    "current_params = nn.get_params_vector()\n",
    "alphas = np.linspace(-1, 1, 50)\n",
    "\n",
    "for idx in [0, len(eigenvalues)//2, -1]:  # Min, middle, max curvature\n",
    "    losses = []\n",
    "    for alpha in alphas:\n",
    "        perturbed = current_params + alpha * eigenvectors[:, idx]\n",
    "        nn.set_params_vector(perturbed)\n",
    "        losses.append(nn.loss(X, y))\n",
    "    nn.set_params_vector(current_params)\n",
    "    \n",
    "    label = f'λ={eigenvalues[idx]:.3f}'\n",
    "    ax.plot(alphas, losses, label=label, linewidth=2)\n",
    "\n",
    "ax.set_xlabel('Step size α')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Loss Along Principal Curvature Directions', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Sorted eigenvalues (log scale)\n",
    "ax = axes[1, 1]\n",
    "sorted_eigs = np.sort(np.abs(eigenvalues))[::-1]\n",
    "ax.semilogy(range(len(sorted_eigs)), sorted_eigs, 'o-', color='purple', markersize=6)\n",
    "ax.set_xlabel('Eigenvalue Rank')\n",
    "ax.set_ylabel('|Eigenvalue| (log scale)')\n",
    "ax.set_title('Curvature Spectrum (Sorted)', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "\n",
    "# 6. Gradient magnitudes\n",
    "ax = axes[1, 2]\n",
    "grad = nn.compute_gradient(X, y)\n",
    "ax.bar(range(len(grad)), np.abs(grad), color='teal', alpha=0.7)\n",
    "ax.set_xlabel('Parameter Index')\n",
    "ax.set_ylabel('|Gradient|')\n",
    "ax.set_title('Gradient Magnitudes', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== KEY INSIGHTS ===\")\n",
    "print(f\"Max eigenvalue (sharpest direction): {np.max(eigenvalues):.4f}\")\n",
    "print(f\"Min eigenvalue (flattest direction): {np.min(eigenvalues):.4f}\")\n",
    "print(f\"Condition number: {np.max(eigenvalues) / (np.abs(np.min(eigenvalues)) + 1e-8):.2f}\")\n",
    "print(f\"Trace (total curvature): {np.sum(eigenvalues):.4f}\")\n",
    "print(f\"\\nGeometry: {'Saddle Point' if np.sum(eigenvalues < 0) > 0 else 'Local Minimum'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4: Training Dynamics\n",
    "\n",
    "Let's train the network and watch how the spectral properties evolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_analyze(X, y, epochs=100, lr=0.05):\n",
    "    \"\"\"\n",
    "    Train the network and track spectral properties.\n",
    "    \"\"\"\n",
    "    nn = ToyNeuralNetwork()\n",
    "    history = {'loss': [], 'max_eigenvalue': [], 'min_eigenvalue': [], 'trace': []}\n",
    "    \n",
    "    print(\"Training neural network...\")\n",
    "    for epoch in range(epochs):\n",
    "        # Forward and backward pass\n",
    "        loss = nn.loss(X, y)\n",
    "        grad = nn.compute_gradient(X, y)\n",
    "        \n",
    "        # Update parameters\n",
    "        params = nn.get_params_vector()\n",
    "        params -= lr * grad\n",
    "        nn.set_params_vector(params)\n",
    "        \n",
    "        history['loss'].append(loss)\n",
    "        \n",
    "        # Compute Hessian spectrum periodically\n",
    "        if epoch % 20 == 0:\n",
    "            eigenvalues, _, _ = analyze_hessian_spectrum(nn, X, y)\n",
    "            history['max_eigenvalue'].append(np.max(eigenvalues))\n",
    "            history['min_eigenvalue'].append(np.min(eigenvalues))\n",
    "            history['trace'].append(np.sum(eigenvalues))\n",
    "            print(f\"Epoch {epoch}: Loss = {loss:.4f}, \"\n",
    "                  f\"λ_max = {np.max(eigenvalues):.4f}, \"\n",
    "                  f\"λ_min = {np.min(eigenvalues):.4f}\")\n",
    "    \n",
    "    return nn, history\n",
    "\n",
    "# Train the network\n",
    "trained_nn, history = train_and_analyze(X, y, epochs=100, lr=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Evolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "epochs_hessian = np.arange(0, len(history['loss']), 20)\n",
    "\n",
    "# Loss curve\n",
    "axes[0, 0].plot(history['loss'], 'b-', linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Loss')\n",
    "axes[0, 0].set_title('Training Loss', fontweight='bold')\n",
    "axes[0, 0].set_yscale('log')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Max and min eigenvalues\n",
    "axes[0, 1].plot(epochs_hessian, history['max_eigenvalue'], 'r-', \n",
    "                linewidth=2, label='λ_max', marker='o')\n",
    "axes[0, 1].plot(epochs_hessian, history['min_eigenvalue'], 'b-', \n",
    "                linewidth=2, label='λ_min', marker='s')\n",
    "axes[0, 1].axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Eigenvalue')\n",
    "axes[0, 1].set_title('Extreme Eigenvalues During Training', fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Trace\n",
    "axes[1, 0].plot(epochs_hessian, history['trace'], 'g-', \n",
    "                linewidth=2, marker='o')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Trace(H)')\n",
    "axes[1, 0].set_title('Hessian Trace (Total Curvature)', fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Sharpness ratio\n",
    "sharpness = np.array(history['max_eigenvalue']) / (np.abs(np.array(history['min_eigenvalue'])) + 1e-8)\n",
    "axes[1, 1].plot(epochs_hessian, sharpness, 'm-', linewidth=2, marker='d')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('λ_max / |λ_min|')\n",
    "axes[1, 1].set_title('Sharpness Ratio (Flat vs Sharp)', fontweight='bold')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].grid(True, alpha=0.3, which='both')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Training dynamics visualization complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5: Attention Mechanism Geometry\n",
    "\n",
    "Now let's apply these concepts to transformer attention - the core of LLMs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAttentionLayer:\n",
    "    \"\"\"\n",
    "    Simplified self-attention mechanism to demonstrate geometric properties.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, d_model=8, n_heads=2, seq_len=4, seed=42):\n",
    "        np.random.seed(seed)\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        self.seq_len = seq_len\n",
    "        \n",
    "        # Query, Key, Value projection matrices\n",
    "        self.W_Q = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_K = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_V = np.random.randn(d_model, d_model) * 0.1\n",
    "        self.W_O = np.random.randn(d_model, d_model) * 0.1\n",
    "        \n",
    "    def attention(self, X):\n",
    "        \"\"\"\n",
    "        Scaled dot-product attention\n",
    "        \"\"\"\n",
    "        Q = X @ self.W_Q\n",
    "        K = X @ self.W_K\n",
    "        V = X @ self.W_V\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = Q @ K.T / np.sqrt(self.d_k)\n",
    "        \n",
    "        # Softmax\n",
    "        attention_weights = self.softmax(scores)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        attended = attention_weights @ V\n",
    "        \n",
    "        # Output projection\n",
    "        output = attended @ self.W_O\n",
    "        \n",
    "        return output, attention_weights, Q, K, V\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "# Create attention layer\n",
    "attention_layer = SimpleAttentionLayer(d_model=8, n_heads=2, seq_len=4)\n",
    "\n",
    "# Input sequence (token embeddings)\n",
    "X_attention = np.random.randn(4, 8) * 0.5\n",
    "\n",
    "# Get attention outputs\n",
    "output, attn_weights, Q, K, V = attention_layer.attention(X_attention)\n",
    "\n",
    "print(\"✓ Attention layer created\")\n",
    "print(f\"   Input shape: {X_attention.shape}\")\n",
    "print(f\"   Attention weights shape: {attn_weights.shape}\")\n",
    "print(f\"   Output shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Attention Geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze spectral properties of attention matrices\n",
    "U_Q, S_Q, Vt_Q = svd(attention_layer.W_Q)\n",
    "U_K, S_K, Vt_K = svd(attention_layer.W_K)\n",
    "U_V, S_V, Vt_V = svd(attention_layer.W_V)\n",
    "U_O, S_O, Vt_O = svd(attention_layer.W_O)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Attention Weights Heatmap\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(attn_weights, cmap='YlOrRd', aspect='auto', vmin=0, vmax=1)\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "ax.set_title('Attention Weight Matrix', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Add values\n",
    "for i in range(attn_weights.shape[0]):\n",
    "    for j in range(attn_weights.shape[1]):\n",
    "        ax.text(j, i, f'{attn_weights[i, j]:.2f}',\n",
    "               ha=\"center\", va=\"center\", color=\"black\", fontsize=8)\n",
    "\n",
    "# 2. QK^T Score Matrix\n",
    "ax = axes[0, 1]\n",
    "QKT = Q @ K.T / np.sqrt(attention_layer.d_k)\n",
    "im = ax.imshow(QKT, cmap='RdBu_r', aspect='auto',\n",
    "               vmin=-np.abs(QKT).max(), vmax=np.abs(QKT).max())\n",
    "ax.set_xlabel('Key Position')\n",
    "ax.set_ylabel('Query Position')\n",
    "ax.set_title('QK^T Score Matrix (Before Softmax)', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 3. Singular Value Spectra\n",
    "ax = axes[0, 2]\n",
    "x_pos = np.arange(len(S_Q))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x_pos - 1.5*width, S_Q, width, label='W_Q', alpha=0.8)\n",
    "ax.bar(x_pos - 0.5*width, S_K, width, label='W_K', alpha=0.8)\n",
    "ax.bar(x_pos + 0.5*width, S_V, width, label='W_V', alpha=0.8)\n",
    "ax.bar(x_pos + 1.5*width, S_O, width, label='W_O', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Singular Value Index')\n",
    "ax.set_ylabel('Singular Value Magnitude')\n",
    "ax.set_title('Singular Value Spectra\\n(Low-Rank Structure!)', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Q Matrix\n",
    "ax = axes[1, 0]\n",
    "im = ax.imshow(Q, cmap='RdBu_r', aspect='auto',\n",
    "               vmin=-np.abs(Q).max(), vmax=np.abs(Q).max())\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Sequence Position')\n",
    "ax.set_title('Query Matrix Q', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 5. K Matrix\n",
    "ax = axes[1, 1]\n",
    "im = ax.imshow(K, cmap='RdBu_r', aspect='auto',\n",
    "               vmin=-np.abs(K).max(), vmax=np.abs(K).max())\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Sequence Position')\n",
    "ax.set_title('Key Matrix K', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# 6. V Matrix\n",
    "ax = axes[1, 2]\n",
    "im = ax.imshow(V, cmap='RdBu_r', aspect='auto',\n",
    "               vmin=-np.abs(V).max(), vmax=np.abs(V).max())\n",
    "ax.set_xlabel('Dimension')\n",
    "ax.set_ylabel('Sequence Position')\n",
    "ax.set_title('Value Matrix V', fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n=== ATTENTION GEOMETRY INSIGHTS ===\")\n",
    "print(f\"W_Q condition number: {S_Q[0] / (S_Q[-1] + 1e-8):.2f}\")\n",
    "print(f\"W_K condition number: {S_K[0] / (S_K[-1] + 1e-8):.2f}\")\n",
    "print(f\"W_V condition number: {S_V[0] / (S_V[-1] + 1e-8):.2f}\")\n",
    "print(f\"W_O condition number: {S_O[0] / (S_O[-1] + 1e-8):.2f}\")\n",
    "print(f\"\\nAttention entropy: {-np.mean(attn_weights * np.log(attn_weights + 1e-10)):.4f}\")\n",
    "print(f\"\\nThis low-rank structure enables LoRA for efficient fine-tuning!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 6: Sharp vs Flat Minima\n",
    "\n",
    "Let's visualize why flat minima generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "theta = np.linspace(-2, 2, 200)\n",
    "epsilon = 0.5  # Perturbation magnitude\n",
    "\n",
    "# Flat minimum: small second derivative\n",
    "flat_minimum = 0.1 + 0.5 * theta**2\n",
    "flat_lambda = 1.0\n",
    "\n",
    "# Sharp minimum: large second derivative\n",
    "sharp_minimum = 0.1 + 5 * theta**2\n",
    "sharp_lambda = 10.0\n",
    "\n",
    "# 1. Flat Minimum\n",
    "ax = axes[0]\n",
    "ax.plot(theta, flat_minimum, 'b-', linewidth=3, label='Loss')\n",
    "ax.scatter([0], [0.1], color='gold', s=300, marker='*', \n",
    "           edgecolors='black', linewidths=2, zorder=5, label='Minimum')\n",
    "ax.axvspan(-epsilon, epsilon, alpha=0.2, color='green', label=f'±{epsilon} perturbation')\n",
    "\n",
    "loss_at_perturb_flat = 0.1 + 0.5 * epsilon**2\n",
    "ax.plot([epsilon, epsilon], [0.1, loss_at_perturb_flat], 'r--', linewidth=2)\n",
    "ax.text(epsilon + 0.1, (0.1 + loss_at_perturb_flat)/2, \n",
    "        f'ΔL={loss_at_perturb_flat-0.1:.2f}',\n",
    "        fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Parameter θ', fontsize=12)\n",
    "ax.set_ylabel('Loss L(θ)', fontsize=12)\n",
    "ax.set_title(f'Flat Minimum (λ = {flat_lambda:.1f})\\n✓ Robust to noise', \n",
    "             fontsize=13, fontweight='bold', color='blue')\n",
    "ax.set_ylim([0, 2])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Sharp Minimum\n",
    "ax = axes[1]\n",
    "ax.plot(theta, sharp_minimum, 'r-', linewidth=3, label='Loss')\n",
    "ax.scatter([0], [0.1], color='gold', s=300, marker='*', \n",
    "           edgecolors='black', linewidths=2, zorder=5, label='Minimum')\n",
    "ax.axvspan(-epsilon, epsilon, alpha=0.2, color='orange', label=f'±{epsilon} perturbation')\n",
    "\n",
    "loss_at_perturb_sharp = 0.1 + 5 * epsilon**2\n",
    "ax.plot([epsilon, epsilon], [0.1, loss_at_perturb_sharp], 'r--', linewidth=2)\n",
    "ax.text(epsilon + 0.1, (0.1 + loss_at_perturb_sharp)/2, \n",
    "        f'ΔL={loss_at_perturb_sharp-0.1:.2f}',\n",
    "        fontsize=10, color='red', fontweight='bold')\n",
    "\n",
    "ax.set_xlabel('Parameter θ', fontsize=12)\n",
    "ax.set_ylabel('Loss L(θ)', fontsize=12)\n",
    "ax.set_title(f'Sharp Minimum (λ = {sharp_lambda:.1f})\\n✗ Sensitive to noise', \n",
    "             fontsize=13, fontweight='bold', color='red')\n",
    "ax.set_ylim([0, 2])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Comparison\n",
    "ax = axes[2]\n",
    "ax.axis('off')\n",
    "\n",
    "comparison_text = f\"\"\"\n",
    "ROBUSTNESS COMPARISON\n",
    "\n",
    "Same perturbation ε = {epsilon}\n",
    "\n",
    "Flat Minimum:\n",
    "  • ΔL = {loss_at_perturb_flat-0.1:.3f}\n",
    "  • Small sensitivity\n",
    "  • ✓ Robust to noise\n",
    "  • ✓ Better generalization\n",
    "\n",
    "Sharp Minimum:  \n",
    "  • ΔL = {loss_at_perturb_sharp-0.1:.3f}\n",
    "  • High sensitivity\n",
    "  • ✗ Sensitive to noise\n",
    "  • ✗ Poor generalization\n",
    "\n",
    "Sensitivity Ratio:\n",
    "{(loss_at_perturb_sharp-0.1)/(loss_at_perturb_flat-0.1):.1f}× more sensitive!\n",
    "\n",
    "KEY INSIGHT:\n",
    "Test data = perturbed training data\n",
    "Flat minima → consistent performance\n",
    "\n",
    "For LLMs:\n",
    "• Use SAM optimizer\n",
    "• Monitor λ_max\n",
    "• Prefer flat architectures\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.05, 0.95, comparison_text, ha='left', va='top', \n",
    "        fontsize=11, family='monospace',\n",
    "        bbox=dict(boxstyle='round', facecolor='lightyellow', \n",
    "                 alpha=0.8, edgecolor='black', linewidth=2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Sharp vs Flat comparison complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary and Key Takeaways\n",
    "\n",
    "### What We've Learned:\n",
    "\n",
    "1. **Hessian Eigenvalues = Optimization Fingerprint**\n",
    "   - λ > 0: Local minimum direction\n",
    "   - λ < 0: Saddle point direction\n",
    "   - |λ| ≈ 0: Flat direction (most important!)\n",
    "\n",
    "2. **Flat Minima Generalize Better**\n",
    "   - Small max eigenvalue → robust to perturbations\n",
    "   - Test data ≈ perturbed training data\n",
    "   - Motivates SAM and other sharpness-aware methods\n",
    "\n",
    "3. **Low-Rank Structure in Attention**\n",
    "   - Singular values decay rapidly\n",
    "   - Only a few dimensions matter\n",
    "   - Enables LoRA for efficient LLM fine-tuning\n",
    "\n",
    "4. **High Dimensions = Saddle Points**\n",
    "   - Almost all critical points are saddles\n",
    "   - Negative eigenvalues are normal\n",
    "   - SGD's noise helps escape saddles\n",
    "\n",
    "5. **Practical Implications**\n",
    "   - Learning rate ∝ 1/λ_max\n",
    "   - Monitor condition number κ = λ_max/λ_min\n",
    "   - Use spectral analysis to debug training\n",
    "   - Design architectures with better geometry\n",
    "\n",
    "### For LLMs Specifically:\n",
    "\n",
    "- **Scale**: Billion parameters → need stochastic spectral approximations\n",
    "- **Architecture**: Attention creates complex but analyzable geometry\n",
    "- **Fine-tuning**: Low-rank structure enables parameter-efficient methods\n",
    "- **Generalization**: Flat minima explain why some models generalize better\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "To explore further:\n",
    "1. Try different network architectures\n",
    "2. Implement SAM optimizer\n",
    "3. Analyze real transformer models\n",
    "4. Study mode connectivity between minima\n",
    "5. Investigate Neural Tangent Kernel connections\n",
    "\n",
    "**The geometric perspective is fundamental to understanding modern AI!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nYou've learned about:\")\n",
    "print(\"  ✓ Loss landscape geometry\")\n",
    "print(\"  ✓ Hessian eigenvalue spectra\")\n",
    "print(\"  ✓ Sharp vs flat minima\")\n",
    "print(\"  ✓ Attention mechanism geometry\")\n",
    "print(\"  ✓ Connections to LLMs\")\n",
    "print(\"\\nThese concepts are at the heart of modern deep learning!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
